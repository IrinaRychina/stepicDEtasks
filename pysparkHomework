from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as spark_sum, to_date

# Создание SparkSession
spark = SparkSession.builder.appName("Read CSV Example").getOrCreate()
# Чтение CSV-файла
df = spark.read.csv("web_server_logs.csv", header=True, inferSchema=True)
# Печать схемы DataFrame
df.printSchema()
# Показ первых 5 строк
df.show(5)


ip_count = df.groupBy("ip").count()
# Сортируем по количеству запросов и берем топ-10
top_10_ips = ip_count.orderBy(col("count").desc()).limit(10)
print(f"Top 10 active IP addresses:")
top_10_ips.show()

method_count = df.groupBy("method").count()
print(f"Request count by HTTP method:")
method_count.show()

not_found_count = df.filter(df.response_code == 404).count()
print(f"Number of 404 responses: {not_found_count}")


df_date = df.withColumn("date", to_date(df["timestamp"]))
df_date_grouped = df_date.groupBy("date").agg(spark_sum("response_size").alias("total_response_size"))
df_date_sorted = df_date_grouped.orderBy("date")
print(f"Total response size by day:")
df_date_sorted.show()
